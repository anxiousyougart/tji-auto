#!/usr/bin/env python3
"""
Test Individual Scrapers

This script tests each scraper individually to identify specific issues
and provides detailed error reporting for debugging.

"""

import subprocess
import sys
import os
import json
import time
from datetime import datetime

def test_scraper(script_name: str, timeout: int = 60) -> dict:
    """
    Test an individual scraper and return detailed results.
    
    Args:
        script_name: Name of the Python script to test
        timeout: Timeout in seconds
        
    Returns:
        Dictionary with test results
    """
    print(f"\nüß™ TESTING: {script_name}")
    print("-" * 50)
    
    start_time = time.time()
    
    try:
        # Run the script
        result = subprocess.run(
            [sys.executable, script_name],
            capture_output=True,
            text=True,
            timeout=timeout,
            cwd=os.getcwd()
        )
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Analyze results
        success = result.returncode == 0
        
        test_result = {
            "script": script_name,
            "success": success,
            "return_code": result.returncode,
            "execution_time": execution_time,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "timeout": False
        }
        
        # Print immediate feedback
        if success:
            print(f"‚úÖ SUCCESS - Completed in {execution_time:.1f}s")
        else:
            print(f"‚ùå FAILED - Return code: {result.returncode}")
            print(f"‚è±Ô∏è  Execution time: {execution_time:.1f}s")
            
            # Show first few lines of error
            if result.stderr:
                error_lines = result.stderr.split('\n')[:3]
                print("üîç Error preview:")
                for line in error_lines:
                    if line.strip():
                        print(f"   {line}")
        
        return test_result
        
    except subprocess.TimeoutExpired:
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"‚è∞ TIMEOUT - Exceeded {timeout}s limit")
        
        return {
            "script": script_name,
            "success": False,
            "return_code": -1,
            "execution_time": execution_time,
            "stdout": "",
            "stderr": f"Timeout after {timeout} seconds",
            "timeout": True
        }
        
    except Exception as e:
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"üí• CRASHED - {str(e)}")
        
        return {
            "script": script_name,
            "success": False,
            "return_code": -2,
            "execution_time": execution_time,
            "stdout": "",
            "stderr": str(e),
            "timeout": False
        }

def check_output_files():
    """Check which output files exist and their sizes."""
    print("\nüìÅ OUTPUT FILE STATUS:")
    print("-" * 50)
    
    expected_files = [
        "../data/ai_selected_article.json",
        "../data/selected_internship.json",
        "../data/selected_job.json", 
        "../data/ai_selected_upskill_article.json",
        "../data/daily_tech_digest.json"
    ]
    
    file_status = {}
    
    for filename in expected_files:
        if os.path.exists(filename):
            size = os.path.getsize(filename)
            print(f"‚úÖ {filename} ({size} bytes)")
            file_status[filename] = {"exists": True, "size": size}
            
            # Try to peek at content
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                    if isinstance(content, dict):
                        if "message" in content and "No suitable content found" in content["message"]:
                            print(f"   ‚ö†Ô∏è  Contains fallback message")
                        else:
                            print(f"   ‚úÖ Contains valid content")
                    file_status[filename]["valid_json"] = True
            except:
                print(f"   ‚ùå Invalid JSON")
                file_status[filename]["valid_json"] = False
        else:
            print(f"‚ùå {filename} (not found)")
            file_status[filename] = {"exists": False, "size": 0}
    
    return file_status

def main():
    """Main testing function."""
    print("üî¨ SCRAPER TESTING SUITE")
    print("=" * 60)
    print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("Testing individual scrapers to identify issues...\n")
    
    # List of scrapers to test
    scrapers = [
        {"script": "demo_tech_news.py", "timeout": 120},
        {"script": "internship_scraper.py", "timeout": 180},
        {"script": "jobs_scraper.py", "timeout": 180},
        {"script": "demo_upskill.py", "timeout": 120},
        {"script": "daily_tech_aggregator.py", "timeout": 60}
    ]
    
    results = []
    
    # Test each scraper
    for scraper in scrapers:
        result = test_scraper(scraper["script"], scraper["timeout"])
        results.append(result)
        
        # Small delay between tests
        time.sleep(2)
    
    # Check output files
    file_status = check_output_files()
    
    # Generate summary report
    print("\nüìä TESTING SUMMARY")
    print("=" * 60)
    
    successful = sum(1 for r in results if r["success"])
    total = len(results)
    
    print(f"‚úÖ Successful: {successful}/{total}")
    print(f"‚ùå Failed: {total - successful}/{total}")
    print(f"‚è±Ô∏è  Total time: {sum(r['execution_time'] for r in results):.1f}s")
    
    # Detailed failure analysis
    failed_scrapers = [r for r in results if not r["success"]]
    
    if failed_scrapers:
        print(f"\nüîç FAILURE ANALYSIS:")
        print("-" * 30)
        
        for failure in failed_scrapers:
            print(f"\n‚ùå {failure['script']}:")
            print(f"   Return code: {failure['return_code']}")
            print(f"   Execution time: {failure['execution_time']:.1f}s")
            
            if failure["timeout"]:
                print(f"   Issue: Timeout")
            elif failure["stderr"]:
                # Show key error lines
                error_lines = failure["stderr"].split('\n')
                key_errors = [line for line in error_lines if any(keyword in line.lower() for keyword in ['error', 'exception', 'failed', 'traceback'])]
                if key_errors:
                    print(f"   Key errors:")
                    for error in key_errors[:3]:  # Show first 3 key errors
                        print(f"     {error.strip()}")
    
    # Recommendations
    print(f"\nüí° RECOMMENDATIONS:")
    print("-" * 30)
    
    if any(r["timeout"] for r in results):
        print("‚è∞ Some scrapers timed out - consider increasing timeout values")
    
    if any("401" in r["stderr"] for r in results):
        print("üîë API authentication issues detected - check Groq API key")
    
    if any("403" in r["stderr"] for r in results):
        print("üö´ Access forbidden errors - some sites may be blocking requests")
    
    if any("import" in r["stderr"].lower() for r in results):
        print("üì¶ Import errors detected - check if all dependencies are installed")
    
    # Save detailed results
    try:
        with open("../data/scraper_test_results.json", "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "summary": {
                    "successful": successful,
                    "total": total,
                    "total_time": sum(r['execution_time'] for r in results)
                },
                "results": results,
                "file_status": file_status
            }, f, indent=2)
        print(f"../data/\nüíæ Detailed results saved to: scraper_test_results.json")
    except Exception as e:
        print(f"\n‚ùå Failed to save results: {e}")

if __name__ == "__main__":
    main()
